{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Gradient Descent ?\n",
    "\n",
    "Gradient Descent is an optimization algorithm used to minimize a function by iteratively moving towards the minimum value of the function. It is widely used in machine learning and deep learning to optimize models, especially neural networks.\n",
    "\n",
    "### Key Concepts of Gradient Descent:\n",
    "\n",
    "1. **Objective Function**:\n",
    "   - Gradient Descent is used to minimize an objective function, typically a loss function in machine learning, which measures how well the model's predictions match the actual data.\n",
    "   \n",
    "2. **Gradients**:\n",
    "   - The gradient is a vector of partial derivatives of the objective function with respect to the model parameters. It points in the direction of the steepest increase of the function. To minimize the function, Gradient Descent updates the parameters in the opposite direction of the gradient.\n",
    "\n",
    "3. **Learning Rate**:\n",
    "   - The learning rate is a hyperparameter that controls the size of the steps taken towards the minimum. If the learning rate is too small, the convergence will be slow. If itâ€™s too large, the algorithm might overshoot the minimum and fail to converge.\n",
    "\n",
    "4. **Iterations**:\n",
    "   - Gradient Descent involves iteratively updating the parameters until the algorithm converges to a minimum, typically when the gradient becomes close to zero or when the decrease in the objective function is below a certain threshold.\n",
    "\n",
    "### Types of Gradient Descent:\n",
    "\n",
    "1. **Batch Gradient Descent**:\n",
    "   - Updates the model parameters using the gradient of the entire dataset. It can be computationally expensive for large datasets but provides a stable convergence.\n",
    "\n",
    "2. **Stochastic Gradient Descent (SGD)**:\n",
    "   - Updates the model parameters using the gradient of a single data point at each iteration. While it can be noisy and less stable, it often converges faster, especially for large datasets.\n",
    "\n",
    "3. **Mini-Batch Gradient Descent**:\n",
    "   - A compromise between Batch Gradient Descent and SGD, it updates the parameters using the gradient computed on a small batch of data. It balances the efficiency and stability of convergence.\n",
    "\n",
    "### Mathematical Formulation:\n",
    "\n",
    "Given a loss function $L(\\theta)$, where $\\theta$ represents the model parameters, the update rule for Gradient Descent is:\n",
    "\n",
    "$$\n",
    "\\theta := \\theta - \\eta \\nabla_{\\theta} L(\\theta)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\eta $ is the learning rate,\n",
    "- $ \\nabla_{\\theta} L(\\theta) $ is the gradient of the loss function with respect to the parameters $\\theta$.\n",
    "\n",
    "### Applications:\n",
    "\n",
    "- **Training Neural Networks**: In deep learning, Gradient Descent is used to optimize the weights and biases of neural networks.\n",
    "- **Linear Regression**: Used to minimize the mean squared error between predictions and actual values.\n",
    "- **Logistic Regression**: Used to find the parameters that minimize the cross-entropy loss.\n",
    "\n",
    "Gradient Descent is a fundamental tool in machine learning and deep learning, enabling the optimization of complex models by efficiently navigating high-dimensional parameter spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
